{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Insincere Questions Classification\n",
    "This notebook uses Pytorch to work on the Kaggle competition  - https://www.kaggle.com/c/quora-insincere-questions-classification .\n",
    "\n",
    "Summary:\n",
    "\n",
    "1. Input contains question id, question text and the target class.\n",
    "target class is \"0\" if it is a sincere question.\n",
    "target class is \"1\" if the question is toxic.\n",
    "Training data contains approximately 13,00,000(+) questions.\n",
    "In that, approximately 80,000(+) questions belong to targe class \"1\".\n",
    "\n",
    "2. Normalizing the Data - All non-char characters are removed from the data.\n",
    "\n",
    "3. Word to Vector is obtained using gensim library.\n",
    "   The corpus contains both train and test questions.\n",
    "   Vector length  = 50.\n",
    "   \n",
    "4. Uses LSTM in combination with CNN.\n",
    "   Optimizer = optim.Adam,lr=0.01,momentum=0.9\n",
    "   Loss = nn.BCEWithLogitsLoss #binary cross entropy loss\n",
    "   \n",
    "   \n",
    "5. As the data size of \"target=1\" is very less compared to that of \"target=0\",\n",
    "   we have done custom sampling. ie., everytime, all of the data for target=1 is included with the equal number of samples from    target=0. The samples are randomly shuffled before training.\n",
    "   \n",
    "6. Confusion Matrix and ROC are used to visualize the prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Statements - all in one place - \n",
    "Pandas is used to read the train/test files. \n",
    "Pytorch is used for the Deep Learning Framework.\n",
    "Scikit Learn is used for Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shree Ganeshaaya Namaha\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "#import nltk\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "#from nltk.corpus import stopwords\n",
    "# Improving the stop words list\n",
    "#nltk.download('stopwords')\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Constants - all in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "time_limit=5#5 hours of time limit.\n",
    "\n",
    "train_file=\"../input/train.csv\"\n",
    "test_file=\"../input/test.csv\"\n",
    "\n",
    "#stop_words = stopwords.words('english')\n",
    "#negation_words = [\"don't\", \"won't\",\"doesn't\",\"couldn't\",\"isn't\",\"wasn't\",\"wouldn't\",\"can't\",\"ain't\",\"shouldn't\",\"not\"]\n",
    "negation_words = [\"don't\", \"won't\",\"doesn't\",\"couldn't\",\"isn't\",\"wasn't\",\"wouldn't\",\"can't\",\"ain't\",\"shouldn't\"]\n",
    "\n",
    "# vector size in word2vec model.\n",
    "# Also, Used in the embedding matrics.\n",
    "EMBEDDING_DIM=100\n",
    "\n",
    "#target classes\n",
    "class_names=[0,1]\n",
    "\n",
    "#Batch size used to train/test.\n",
    "BATCH_SIZE=4000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Purpose Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the timestamp along with the log message\n",
    "def myprint(*argv):\n",
    "    sstr=datetime.now()\n",
    "    for arg in argv:\n",
    "        sstr=\"{} {}\".format(sstr,arg)\n",
    "    print(sstr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following block is not used at pesent, as it takes too long to pos-tag and stemming the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\"\"\"        \n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "## Does the following - \n",
    "## 1) replace the org names and person names with a common word.\n",
    "## In that way the training will not bias based on person/org.\n",
    "## And also, the text corpus will reduce and model can work on important words only.\n",
    "## 2) the continous chunks / couplets will be joined using \"_\" and will be treated as one word. \n",
    "## 3) removes the stop-words, again to reduce the corpus to important words.\n",
    "## 4) stemming - let us use the base state of the word - again to reduce the corpus to few important words only.\n",
    "\n",
    "\n",
    "def reduce_to_important_words_only(line):\n",
    "    p_names,org_names,chunks=continuous_chunks(line)\n",
    "    for chunk in chunks:\n",
    "        tmp=re.sub(\" \",\"_\",chunk)\n",
    "        try:\n",
    "            line=re.sub(chunk, tmp, line)\n",
    "        except Exception as e:\n",
    "            print(\"chunk=\",chunk,str(e) )\n",
    "    for chunk in p_names:\n",
    "        try:\n",
    "            line=re.sub(chunk, \"PERSON_NAME\", line)\n",
    "        except Exception as e:\n",
    "            print(\"chunk=\",chunk,str(e) )\n",
    "\n",
    "    for chunk in org_names:\n",
    "        try:\n",
    "            line=re.sub(chunk, \"ORG_NAME\", line)\n",
    "        except Exception as e:\n",
    "            print(\"chunk=\",chunk,str(e) )                \n",
    "\n",
    "    \n",
    "    line = line.lower()\n",
    "    word_list = line.split(' ')\n",
    "    newword_list = []\n",
    "    prev_word = ''\n",
    "\n",
    "    for word in word_list:\n",
    "        word=word.strip()\n",
    "        ### get the root form of the word\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "\n",
    "        if word in [\"person_name\",\"org_name\"]:\n",
    "            newword_list.append(word)\n",
    "            continue\n",
    "\n",
    "        if word in negation_words:\n",
    "            prev_word = word\n",
    "            continue\n",
    "\n",
    "        ### LEMMATIZE\n",
    "        #word= lemmatizer.lemmatize(word)\n",
    "        word= stemmer.stem(word)\n",
    "\n",
    "        if prev_word in negation_words:                \n",
    "            word = 'not_' + word                        \n",
    "            newword_list.append(word)                        \n",
    "            prev_word = ''\n",
    "        else:\n",
    "            newword_list.append(word)\n",
    "\n",
    "    line = ' '.join(newword_list)\n",
    "    return line\n",
    "    \n",
    "def continuous_chunks(line):\n",
    "    chunked = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(line)))\n",
    "    continuous_chunk = []\n",
    "    person_names=[]\n",
    "    org_names=[]\n",
    "    for i in chunked:\n",
    "        ttype=i.__class__.__name__\n",
    "        if ttype == \"Tree\":\n",
    "            if i.label()== \"PERSON\":\n",
    "                person_names.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "            elif i.label()== \"ORGANIZATION\":\n",
    "                org_names.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "            else:\n",
    "                if len(i.leaves()) > 1:\n",
    "                    continuous_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "\n",
    "        elif ttype == \"Tuple\":\n",
    "            if i[1] == \"PERSON\":\n",
    "                person_names.append(i[0])\n",
    "            elif i[1] == \"ORGANIZATION\":                    \n",
    "                org_names.append(i[0])\n",
    "\n",
    "             #elif current_chunk:\n",
    "             #        named_entity = \" \".join(current_chunk)\n",
    "             #        if named_entity not in continuous_chunk:\n",
    "             #                continuous_chunk.append(named_entity)\n",
    "             #                current_chunk = []\n",
    "             #else:\n",
    "             #        continue\n",
    "    return person_names, org_names, continuous_chunk \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only alphabetic/text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "\n",
    "def normalize_text(filename):\n",
    "    myprint(\"normalize file \", filename)\n",
    "    df=pd.read_csv(filename)\n",
    "    arr=[]\n",
    "    for i,row in df.iterrows():\n",
    "        qid=row[\"qid\"]\n",
    "        qn=normalize_line(row[\"question_text\"])\n",
    "        if \"target\" in row:\n",
    "            target=row[\"target\"]\n",
    "            tple=(qid,qn,target)\n",
    "        else:\n",
    "            tple=(qid,qn)\n",
    "        arr.append(tple)\n",
    "        #df.set_value(i,\"question_text\",v)\n",
    "        if (i%100000==0):\n",
    "            myprint(i)\n",
    "    return arr\n",
    "\n",
    "def normalize_line(line):\n",
    "    line=str(line).lower()\n",
    "    for word in negation_words:\n",
    "        line=re.sub(word,\"not\",line)\n",
    "        \n",
    "    line = re.sub(r'\\W', ' ', str(line))\n",
    "    line = re.sub(r'\\d', ' ', line)\n",
    "\n",
    "    line = re.sub(r'br[\\s$]', ' ', line)\n",
    "    line = re.sub(r'\\s+[a-z][\\s$]', ' ',line)\n",
    "    line = re.sub(r'b\\s+', '', line)\n",
    "    line = re.sub(r'\\s+', ' ', line)\n",
    "    line=line.strip()\n",
    "    arr=line.split(\" \")\n",
    "    \n",
    "    tokens=[]\n",
    "    for w in arr:\n",
    "        w=w.strip()\n",
    "        if len(w)>0:\n",
    "            #w=stemmer.stem(w)\n",
    "            tokens.append(w)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-PROCESS/NORMALIZE THE QUESTION TEXT TO REMOVE UN-NECESSARY CONTENT.\n",
    "The output of this is - \n",
    "train_arr_no_gram is an array of a tuple like this - ('00014894849d00ba98a9', ['my', 'voic', 'rang', 'is', 'my', 'chest', 'voic', 'goe', 'up', 'to', 'includ', 'sampl', 'in', 'my', 'higher', 'chest', 'rang', 'what', 'is', 'my', 'voic', 'type'],0)\n",
    "ie., array of (question_id,[array question text tokens],question_type).\n",
    "\n",
    "test_arr_no_gram is an array of a tuple like this - ('000032939017120e6e44', ['do', 'you', 'have', 'an', 'adopt', 'dog', 'how', 'would', 'you', 'encourag', 'peopl', 'to', 'adopt', 'and', 'not', 'shop'])\n",
    "ie., array of (question_id,[array question text tokens])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myprint(\"normalize train-start\")\n",
    "train_arr_no_gram=normalize_text(train_file)\n",
    "myprint(\"normalize train-done\")\n",
    "\n",
    "myprint(\"normalize test-start\")\n",
    "test_arr_no_gram=normalize_text(test_file)\n",
    "myprint(\"normalize test-done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram\n",
    "Let us use 3-gram, this will give bit denser data to train.\n",
    "Sample entry of train_arr before n_gram operation is - \n",
    "['00002165364db923c7e6', ['how', 'did', 'quebec', 'nationalist', 'see', 'their', 'provinc', 'as', 'nation', 'in', 'the'], 0]\n",
    "\n",
    "the same after n_gram operation is - \n",
    "['00002165364db923c7e6',['how', 'did', 'quebec', 'did', 'quebec', 'nationalist', 'quebec', 'nationalist', 'see', 'nationalist', 'see', 'their', 'see', 'their', 'provinc', 'their', 'provinc', 'as', 'provinc', 'as', 'nation', 'as', 'nation', 'in', 'nation', 'in', 'the'],0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens contains an array of words.\n",
    "def tokenize_n_gram(tokens,n):\n",
    "    arr_gram=[]\n",
    "    pointer=0\n",
    "    cnt=0\n",
    "    done=False\n",
    "    lth=len(tokens)\n",
    "    while done==False:\n",
    "        if (pointer+n) > lth:\n",
    "            done=True\n",
    "            break\n",
    "        arr_gram.append(tokens[pointer+cnt])\n",
    "        cnt+=1\n",
    "        if cnt == n:\n",
    "            cnt=0\n",
    "            pointer+=1\n",
    "    return arr_gram\n",
    "\n",
    "myprint(\"start n_gram for train_arr\")\n",
    "train_arr=[]\n",
    "for (qid,tokens,qtype) in train_arr_no_gram:\n",
    "    n_gramed=tokenize_n_gram(tokens,3)\n",
    "    train_arr.append((qid,n_gramed,qtype))\n",
    "    \n",
    "myprint(\"start n_gram for test_arr\")    \n",
    "test_arr=[]\n",
    "for (qid,tokens) in test_arr_no_gram:\n",
    "    n_gramed=tokenize_n_gram(tokens,3)\n",
    "    test_arr.append((qid,n_gramed))\n",
    "myprint(\"done n-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#with open(\"train_arr_norm\", 'wb') as handle:\n",
    "#    pickle.dump(train_arr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open(\"test_arr_norm\", 'wb') as handle:\n",
    "#    pickle.dump(test_arr, handle, protocol=pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"train_arr_norm\", 'rb') as handle:\n",
    "#    train_arr = pickle.load(handle)\n",
    "#with open(\"test_arr_norm\", 'rb') as handle:\n",
    "#    test_arr = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE THE CORPUS OF WORDS from TRAIN and TEST sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lth_train=len(train_arr)\n",
    "lth_test=len(test_arr)\n",
    "corpus=np.concatenate((np.array([qn for (i,qn,t) in train_arr]), np.array([qn for (i,qn) in test_arr])))\n",
    "lth_total=len(corpus)\n",
    "print(\"lth_train=\",lth_train,\",lth_test=\",lth_test,\",total=\",lth_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE THE WORD2VEC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myprint(\"start word2vec\")\n",
    "gw2c = Word2Vec(corpus, size=EMBEDDING_DIM, window=5, min_count=1, workers=4)\n",
    "myprint(\"end word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE ARE NOT USING THE FOLLOWING FUNCTION AS WE CREATE OUR OWN WORD VECTORS USING GENSIM IN THE ABOVE STEP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"   \n",
    "def load_standard_word2vec(self): \n",
    "\n",
    "    #ffile=\"embeddings\\\\glove.840B.300d.txt\"\n",
    "    ffile=\"..\\\\embeddings\\\\GoogleNews-vectors-negative300.bin\"\n",
    "    #ffile=\"embeddings\\\\paragram_300_sl999.txt\"\n",
    "    #ffile=\"embeddings\\\\wiki-news-300d-1M.vec\"\n",
    "\n",
    "    print(\"loading embedding word2vec file \",ffile)\n",
    "\n",
    "    self.word2vec = gensim.models.KeyedVectors.load_word2vec_format(ffile, binary=True)  \n",
    "    self.EMBEDDING_DIM=300\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECIDE ON THE MAX NUMBER OF WORDS TO BE CONSIDERED IN EACH QUESTION.\n",
    "Steps for this - \n",
    "    Get the length vector - contains lengths of each question in the corpus.\n",
    "    Sort lengths based on ascending order.\n",
    "    If the MAX length is too big compared to the length of the 95% of the data, then consider the length of the 95% of the data.     ie., we don't want to waste our calculation on the 5% of the questions if they are too big.\n",
    "    But, we want to cover at the least - all the words in 95% of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_sent_lth(corpus):   \n",
    "    myprint(\"start find_max_sent_lth\")\n",
    "    #we want to take the max_sent_lth as the length of the 95% data.\n",
    "    lths=[len(arr) for arr in corpus]\n",
    "    lths=np.sort(lths)    \n",
    "    mmax=lths[-1]         \n",
    "    mmax_95_perc=lths[int(len(lths)*0.95)]\n",
    "    diff=mmax-mmax_95_perc\n",
    "    mmax_5_perc=lths[int(len(lths)*0.05)]\n",
    "    print(\"max lth=\",mmax,\",max_95_perc=\",mmax_95_perc,\",max_5_perc=\",mmax_5_perc,\", diff=\",diff)\n",
    "    if diff > mmax_5_perc:    \n",
    "        ret=mmax_95_perc\n",
    "    else:\n",
    "        ret=mmax\n",
    "    \n",
    "    myprint(\"end find_max_sent_lth=\",ret)\n",
    "    return ret\n",
    "\n",
    "MAX_SENTENCE_LENGTH=find_max_sent_lth(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILDING WORD INDEX - This is done manually, as gensim word2vec gives us an array, but we want to use a dictionary object for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Following code is for manually doing it, however, is not used currently.\n",
    "def build_word_index(corpus):\n",
    "    myprint(\"start build_word_index\")\n",
    "    word2index={} #SOS_token:0, EOS_token:1}\n",
    "    index2word={}#0:SOS_token,1:EOS_token}\n",
    "    num_words=0\n",
    "    for tokens in corpus:\n",
    "        for token in tokens:\n",
    "            if token not in word2index:\n",
    "                word2index[token]=num_words\n",
    "                index2word[num_words]=token\n",
    "                num_words+=1\n",
    "    myprint(\"end build_word_index, num_words=\",num_words)\n",
    "    return word2index,index2word,num_words\n",
    "\n",
    "word2index,index2word,num_words=build_word_index(corpus)\n",
    "\"\"\"\n",
    "tmparr=gw2c.wv.index2word\n",
    "word2index={}\n",
    "index2word={}\n",
    "for i,w in enumerate(tmparr):\n",
    "    word2index[w]=i\n",
    "    index2word[i]=w\n",
    "num_words=len(word2index)\n",
    "\n",
    "print(\"num_words=\",num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequences_from_tokens function does following - \n",
    "1) Takes the array of tokens/words as input and returns the array of corresponding indexes from word2index.\n",
    "2) Also, this ensures that the returned array is of length=MAX_SENTENCE_LENGTH.\n",
    "    ie., if the tokens arr is bigger, it truncates; and pads if the arr is shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_from_tokens(tokens_arr):\n",
    "    ret=[]\n",
    "    for arr in tokens_arr:\n",
    "        ind_arr=[]\n",
    "        if len(tokens_arr) > MAX_SENTENCE_LENGTH:\n",
    "            tokens_arr=tokens_arr[0:MAX_SENTENCE_LENGTH]\n",
    "        tokens_arr=tokens_arr\n",
    "        for word in arr:\n",
    "            ind_arr.append(word2index[word])\n",
    "        \n",
    "        ind_arr=F.pad(torch.FloatTensor(ind_arr),(0,MAX_SENTENCE_LENGTH-len(ind_arr)),\"constant\",0).numpy()[:]\n",
    "        \n",
    "        ret.append(ind_arr)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILD EMBEDDING MATRIX - This will contain all words in the corpus in the vector form.\n",
    "This MATRIX is used as the first layer in the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embed_matrix(lword2idx,lwv):\n",
    "    # prepare embedding matrix\n",
    "    myprint('Filling pre-trained embeddings...')\n",
    "\n",
    "    #index in word2idx  starts from 1.\n",
    "    embedding_matrix = np.zeros((len(lword2idx)+1, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in lword2idx.items():\n",
    "\n",
    "        try:\n",
    "            embedding_vector = lwv[word]        \n",
    "        except:# Exception as e:\n",
    "            #print (e)\n",
    "            #if word is not found in word2vec - \n",
    "            embedding_vector=np.zeros(EMBEDDING_DIM)\n",
    "            for indx in range(0,50):\n",
    "                embedding_vector[indx]=i\n",
    "            #also that partuclar position\n",
    "            #embedding_vector[i]=i\n",
    "        embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embed_matrix=build_embed_matrix(word2index,gw2c.wv)\n",
    "print(embed_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS OUR NEURAL NETWORK MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):    \n",
    "\n",
    "    def create_embed_layer(self,numwords, EMBEDDING_DIM, embedding_matrix):    \n",
    "        # load pre-trained word embeddings into an Embedding layer\n",
    "        # note that we set trainable = False so as to keep the embeddings fixed\n",
    "    \n",
    "        #torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, \n",
    "        #sparse=False, _weight=None)\n",
    "        embedding_layer = nn.Embedding(\n",
    "          numwords+1,\n",
    "          EMBEDDING_DIM)\n",
    "    \n",
    "        embedding_layer.load_state_dict({'weight': torch.FloatTensor(embedding_matrix)})\n",
    "        ## or do this - embedding_layer.weight.data.copy_(pretrained_embeddings)\n",
    "    \n",
    "        #we do not want to update the pretrained embeddings. so set requires_grad to False.\n",
    "        embedding_layer.weight.requires_grad = False\n",
    "        return embedding_layer\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self,numwords,EMBEDDING_DIM,embedding_matrix):\n",
    "        super(NNModel,self).__init__()\n",
    "        \n",
    "        op_classes=2        \n",
    "        self.embedding=self.create_embed_layer(numwords,EMBEDDING_DIM,embedding_matrix)\n",
    "        self.conv1_kernel_size=3\n",
    "        \n",
    "        self.rnn_hidden_size=15\n",
    "        self.rnn_num_layers=3\n",
    "        \n",
    "        #considers trigrams\n",
    "        #Make sure in_channels is 1, as we are doing \n",
    "        #this before we run conv2d - embed.unsqueeze(1)#change shape to - [batch size, 1, max_sentence_length, emb_dim]\n",
    "        #so, 2nd dimension of the input is 1, that means in_channels should also be 1.\n",
    "        self.conv1 = nn.Conv1d(EMBEDDING_DIM,#input size\n",
    "                               self.rnn_hidden_size,#output size\n",
    "                               self.conv1_kernel_size)\n",
    "        \n",
    "        \n",
    "        ### NOW CREATE A GRU BI DIRECTIONAL NETWORK, with a small dropout.\n",
    "        #self.gru = nn.GRU(EMBEDDING_DIM, self.gru_hidden_size, num_layers=self.gru_num_layers, bidirectional=True, \n",
    "        #                   dropout=0.1)\n",
    "        self.lstm = nn.LSTM(self.rnn_hidden_size, self.rnn_hidden_size, num_layers=self.rnn_num_layers, bidirectional=True, \n",
    "                           dropout=0.1, batch_first=True)\n",
    "        \n",
    "        \n",
    "        #self.rnn=nn.RNN(EMBEDDING_DIM,self.hidden_nodes)\n",
    "        #self.rnn=nn.LSTM(EMBEDDING_DIM,self.hidden_nodes,num_layers=n_layers,bidirectional=self.bidirectional,dropout=dropout)\n",
    "        \n",
    "        \n",
    "        #from  bidirectional LSTM - we will consider the last two layers - ie., last forward layer and last backwar layer output.\n",
    "        #that's why the input to next layer will become hidden_nodes*2\n",
    "        self.fc=nn.Linear(self.rnn_hidden_size*2,op_classes)\n",
    "        \n",
    "        self.figcnt=0\n",
    "        self.class_names=[0,1]\n",
    "        \n",
    "        \n",
    "    #def forward(self, indata,gru_h):   \n",
    "    def forward(self,indata,h,c):\n",
    "        #[batch_size x WORD_SEQ_IN_SENTENCE]\n",
    "        indata=torch.LongTensor(indata)\n",
    "        \n",
    "        embed=self.embedding(indata)        \n",
    "        #[batch size, sentence length, emb dim]\n",
    "        #packed=torch.nn.utils.rnn.pack_padded_sequence(embed2,input_lengths)\n",
    "        \n",
    "        ############# CONVOLUTIONAL LAYER ##################\n",
    "        #convert batch_size x sentence_lth x emb_dim into batch_size x emb_dim x sentence_length\n",
    "        conv1_input=embed.transpose(1,2)                        \n",
    "        conv1_out=self.conv1(conv1_input)\n",
    "        max_pooled = F.max_pool1d(conv1_out, 3)\n",
    "        \n",
    "        #convert batch_size x hidden_size x (modified sent length) to batch_size x (modified sent length) x hidden_size\n",
    "        rnn_in=max_pooled.transpose(1,2)\n",
    "        \n",
    "        ######## RNN #######################\n",
    "\n",
    "        if h is None:\n",
    "            h = torch.zeros(self.rnn_num_layers*2, rnn_in.shape[0], self.rnn_hidden_size)\n",
    "            c = torch.zeros(self.rnn_num_layers*2, rnn_in.shape[0], self.rnn_hidden_size)\n",
    "            #h = torch.randn(self.gru_num_layers*2, conv1.shape[1], self.gru_hidden_size)\n",
    "            #c = torch.randn(self.gru_num_layers*2, conv1.shape[1], self.gru_hidden_size)\n",
    "\n",
    "        #output,gru_h=self.gru(embed,gru_h)        \n",
    "        rnn_out, (h,c)=self.lstm(rnn_in,(h,c))\n",
    "        \n",
    "        #gru_hidden_state is of shape (num_layers * num_directions, batch, hidden_size): \n",
    "        #if we have 2 layers, (-1,..) gives the output from the previous run\n",
    "        # and  (-2,..) gives the output from the previous-to-previous run.\n",
    "        \n",
    "        ##concat the data from the last forward layer and the last backward layer.\n",
    "        #cat=torch.cat((gru_h[-2,:,:],gru_h[-1,:,:]),dim=1)\n",
    "        cat=torch.cat((h[-2,:,:],h[-1,:,:]),dim=1)\n",
    "                \n",
    "        out = self.fc(cat)\n",
    "        \n",
    "        \n",
    "        #print(\"input=\",indata.shape)\n",
    "        #print(\"embed=\",embed.shape)\n",
    "        #print(\"conv1_input=\",conv1_input.shape)\n",
    "        #print(\"conv1_out=\",conv1_out.size())\n",
    "        #print(\"conv1 after maxpool=\",max_pooled.shape)\n",
    "        #print(\"hidden shape=\",h.shape)        \n",
    "        #print(\"hidden-cat shape=\",cat.shape)\n",
    "        #print(\"out shape=\",out.shape)        \n",
    "\n",
    "        #input= torch.Size([4000, 72])\n",
    "        #embed= torch.Size([4000, 72, 100])\n",
    "        #conv1_input= torch.Size([4000, 100, 72])\n",
    "        #conv1_out= torch.Size([4000, 10, 70])\n",
    "        #conv1 after maxpool= torch.Size([4000, 10, 23])\n",
    "        #hidden shape= torch.Size([6, 4000, 10])\n",
    "        #hidden-cat shape= torch.Size([4000, 20])\n",
    "        #out shape= torch.Size([4000, 2])\n",
    "\n",
    "        return out, h, c\n",
    "\n",
    "    def round_prediction(self,preds_sigmoid):    \n",
    "        #round predictions to the closest integer\n",
    "        #preds=np.array([0 if i<0.7 else 1 for i in preds_sigmoid])\n",
    "        preds=np.array([0 if i>j else 1 for [i,j] in preds_sigmoid])\n",
    "        return preds\n",
    "        \n",
    "    def binary_accuracy(self,rounded_preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "        #round predictions to the closest integer\n",
    "        \n",
    "        correct = (torch.Tensor(rounded_preds) == torch.Tensor(y)).float() #convert into float for division     \n",
    "        acc = correct.sum()/len(correct)\n",
    "        return acc, correct.sum().item()\n",
    "    \n",
    "\n",
    "    def plot_metrics(self,y_pred_rounded,y_pred,batch_target):                        \n",
    "\n",
    "        # Plot non-normalized confusion matrix\n",
    "        plt.figure(self.figcnt)\n",
    "        self.figcnt+=1\n",
    "        self.plot_confusion(batch_target,y_pred_rounded) \n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(batch_target, y_pred_rounded, pos_label=2)\n",
    "        myprint(\"fpr=\",fpr,\", tpr=\",tpr,\",thresholds=\",thresholds)\n",
    "    \n",
    "    \n",
    "    def plot_confusion(self,\n",
    "             batch_target,\n",
    "             pred_rounded,\n",
    "             classes=[0,1],\n",
    "             normalize=False,\n",
    "             title='Confusion matrix',\n",
    "             cmap=plt.cm.Blues):\n",
    "\n",
    "        cm=confusion_matrix(batch_target, pred_rounded)#, labels=class_names)\n",
    "        print(cm)\n",
    "\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            print(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "            print('Confusion matrix, without normalization')\n",
    "    \n",
    "        \n",
    "    \n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "    \n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_prediction(self,y_pred_sigmoid,y_pred_rounded):\n",
    "        y=[]\n",
    "        cnt=1\n",
    "        for i in y_pred_sigmoid:\n",
    "            y.append(cnt)\n",
    "            cnt=cnt+1 \n",
    "    \n",
    "        plt.figure(self.figcnt)\n",
    "        plt.title(\"sigmoid_{}\".format(self.figcnt))\n",
    "        #plt.scatter(y,y_pred_sigmoid,s=5)\n",
    "        x=[i for [i,j] in y_pred_sigmoid]\n",
    "        y=[j for [i,j] in y_pred_sigmoid]\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "        plt.scatter(x,y,s=5)\n",
    "        plt.show()\n",
    "        self.figcnt+=1\n",
    "                        \n",
    "        #plt.figure(self.figcnt)\n",
    "        #plt.title(\"rounded_{}\".format(self.figcnt))\n",
    "        #plt.scatter(y,y_pred_rounded,s=5)\n",
    "        #plt.show()\n",
    "        #self.figcnt+=1\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is just to get an idea on how many class-0 and class-1 items are present in train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0=[(qn,cls) for (qid,qn,cls) in train_arr if cls==0]\n",
    "class_1=[(qn,cls) for (qid,qn,cls) in train_arr if cls==1]\n",
    "lth_0=len(class_0)\n",
    "lth_1=len(class_1)\n",
    "\n",
    "train_qn_cls=[(qn,cls) for (qid,qn,cls) in train_arr]\n",
    "        \n",
    "print(\"Total train data belonging to class_0=\",lth_0)\n",
    "print(\"Total train data belonging to class_1=\",lth_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate Training Set\n",
    "There are only 80810 class_1 samples , and 1225312 class_0 samples.\n",
    "let us upsample it. ie., make the class-1 samples equal to class-2.\n",
    "we can use \"from sklearn.utils import resample\" for this. \n",
    "But, i saw that it repeats the same sample many times.\n",
    "So we are doing it manually here and using np.random.shuffle to randomize the spread of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=len(class_0)-len(class_1)\n",
    "print(\"diff=\",diff)\n",
    "total=len(class_0)+len(class_1)\n",
    "\n",
    "j=0\n",
    "for i in range (0,diff):    \n",
    "    if  i % 50000 == 0:\n",
    "        print (i)\n",
    "    pos=np.random.randint(0,len(train_qn_cls))\n",
    "    \n",
    "    #insert class1 sample randomly into train_arr.\n",
    "    train_qn_cls.insert(pos,class_1[j])\n",
    "    \n",
    "    j+=1\n",
    "    if j >= len(class_1):\n",
    "        j=0\n",
    "    if len(train_qn_cls) == total:\n",
    "        break\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model, optimizer and loss function.\n",
    "Intialize the hidden weights of GRU (gru_h) to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NNModel(num_words,EMBEDDING_DIM,\n",
    "          embed_matrix)\n",
    "#optimizer=optim.SGD(model.parameters(),lr=0.01,momentum=0.9)\n",
    "#optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "lr=0.01\n",
    "optimizer=optim.Adam(model.parameters(), lr=lr)\n",
    "#decay LR by a factor of 0.1 every 2 epochs\n",
    "scheduler=lr_scheduler.StepLR(optimizer,step_size=1,gamma=0.1)\n",
    "\n",
    "\n",
    "#loss_fn=torch.nn.MSELoss(reduction=\"sum\")\n",
    "#loss_fn=nn.CrossEntropyLoss()#we don't use this here as it is btter suited for multi class classificaiton.\n",
    "\n",
    "#BCEWithLogitsLoss combines the sigmoid layer and the BCELoss in one single class but is numerically more stable and hence, should be preferred.\n",
    "#Note here that you don’t need to pass the input tensor to the sigmoid layer before training with BCEWithLogitsLoss.\n",
    "loss_fn=nn.BCEWithLogitsLoss()#binary cross entropy loss\n",
    "gru_h=None\n",
    "h=None\n",
    "c=None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_batch_from_chunk() function will extract the batch of questions and targets from the given array.\n",
    "\n",
    "For the last batch in the array, the number of elements may be smaller than the batch size.\n",
    "In that case, we append the starting elements of the train set to make up the batch size.\n",
    "This is because, we want to reuse the last calculated gru_h that is of the dimension of batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_batch_from_chunk(sequences_chunk,targets_chunk,start,end):\n",
    "    lth=len(sequences_chunk)\n",
    "    if end > lth:\n",
    "        #this last batch is lesser than the standard batch size.\n",
    "        #we add the initial question set to make this batch of standard size.\n",
    "        batch_input=sequences_chunk[start:lth]\n",
    "        batch_target=targets_chunk[start:lth]\n",
    "\n",
    "        #fill the remaining with the questions from first.\n",
    "        remain_input=sequences_chunk[0:end-lth]\n",
    "        remain_target=targets_chunk[0:end-lth]\n",
    "\n",
    "        batch_input=np.concatenate((batch_input,remain_input))\n",
    "        batch_target=np.concatenate((batch_target,remain_target))\n",
    "\n",
    "    else:            \n",
    "        batch_input=sequences_chunk[start:end]\n",
    "        batch_target=targets_chunk[start:end]\n",
    "\n",
    "    return batch_input, batch_target\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the core training Loop - \n",
    "  1) calls models forward() function\n",
    "  2) calculates loss\n",
    "  3) propagates the loss backwards\n",
    "  4) steps up the weight values using the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_loop(batch_input,batch_target,gru_h,cnt):    \n",
    "def train_loop(batch_input,batch_target,h,c,cnt): \n",
    "    \n",
    "    class_0_batch=[cls for cls in batch_target if cls==0]\n",
    "    class_1_batch=[cls for cls in batch_target if cls==1]\n",
    "    myprint(\"target len cls0=\",len(class_0_batch),\" cls1=\",len(class_1_batch))\n",
    "\n",
    "    batch_target_2d=[[1,0] if cls==0 else [0,1] for cls in batch_target]\n",
    "    batch_target_2d=torch.Tensor(batch_target_2d)\n",
    "        \n",
    "    #rnn_batch_input=torch.LongTensor(np.transpose(cnn_batch_input))\n",
    "    # batch_input=torch.LongTensor(batch_input)\n",
    "            \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #myprint( \"rnn start\")\n",
    "    #y_pred_sigmoid,gru_h=model(batch_input,gru_h)\n",
    "    #y_pred,gru_h=model(batch_input,gru_h)\n",
    "    \n",
    "    y_pred,h,c=model(batch_input,h,c)\n",
    "    #myprint(\"rnn loss\")\n",
    "    \n",
    "    #our loss function BCEWithLogitsLoss calculates the sigmoid from the given prediction.\n",
    "    #So, we just pass the raw predicted values.\n",
    "    loss=loss_fn(y_pred,batch_target_2d)\n",
    "    loss_num=loss.item()  \n",
    "    #myprint(\"rnn backward\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    ## detach the gru from the previous history/graph.\n",
    "    ## Otherwise, it will hog the memory.\n",
    "    #gru_h.detach_()\n",
    "    h.detach_()\n",
    "    c.detach_()\n",
    "    \n",
    "    ## Calculate the sigmoid value of hte probability of the two classes.\n",
    "    y_pred_sigmoid= torch.sigmoid(y_pred)\n",
    "\n",
    "    ## For ex. if class1 probability is more than that of class 0, \n",
    "    #then we predict that this sample belongs to class 1.\n",
    "    y_pred_rounded=model.round_prediction(y_pred_sigmoid)\n",
    "            \n",
    "    ## This is just an informational log.\n",
    "    class_0_pred=[cls for cls in y_pred_rounded if cls==0]\n",
    "    class_1_pred=[cls for cls in y_pred_rounded if cls==1]\n",
    "    myprint(\"predicted len cls0=\",len(class_0_pred),\" cls1=\",len(class_1_pred))\n",
    "             \n",
    "    acc,num_correct = model.binary_accuracy(y_pred_rounded, batch_target)\n",
    "    \n",
    "    ## Let us plot sigmoid values and confusion metrics for some batches in between.\n",
    "    ## we can't plot for all batches, as the browser will collapse due to increased output logs.\n",
    "    if cnt % 15 == 0:\n",
    "        model.plot_prediction( y_pred_sigmoid.detach().numpy(),y_pred_rounded)\n",
    "        model.plot_metrics(y_pred_rounded, y_pred_sigmoid.detach().numpy(),batch_target)\n",
    "    #else:\n",
    "    #    cm=confusion_matrix(batch_target, y_pred_rounded)#, labels=class_names)\n",
    "    #    print(cm)\n",
    "    return loss_num, num_correct, h , c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_fn() function does all the nuances like creating the batch, shuffling the data, and calls the core train_loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(class_0)\n",
    "\n",
    "\n",
    "\"\"\"if epoch == 0:\n",
    "lr=0.01\n",
    "elif epoch == 1:\n",
    "lr=0.001\n",
    "else:\n",
    "lr=0.0001\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def train_fn(end_epoch): \n",
    "\n",
    "    #set the mode to training.\n",
    "    model.train()\n",
    "    h=None\n",
    "    c=None\n",
    "    \n",
    "    #Each epoch should cover all elements of class_0 and of class_1.\n",
    "    for epoch in range(0,end_epoch):\n",
    "        if epoch < 2:\n",
    "            scheduler.step()        \n",
    "        epoch_loss=0\n",
    "        #epoch_accuracy=0\n",
    "        cnt=0\n",
    "        total=0\n",
    "        correct=0\n",
    "        gru_h=None\n",
    "        #### as the class_1 items are too few when compared to class_0, \n",
    "        #### we take the chunk, that contains equal number of class_0 and class_1 item for training\n",
    "        #### we continue training the chunk, till we cover all items in class_0.\n",
    "        #start_chunk=0\n",
    "        #end_chunk=lth_1        \n",
    "        #done_chunks=False\n",
    "        #chunk_num=-1\n",
    "        \n",
    "        myprint(\"epoch-\",epoch, \" start shuffle\")\n",
    "        for i in range (0,10):                    \n",
    "            np.random.shuffle(train_qn_cls)\n",
    "        myprint(\"epoch-\",epoch, \" end shuffle\")\n",
    "        print(optimizer.defaults)\n",
    "            \n",
    "        qn_epoch=[]\n",
    "        target_epoch=[]\n",
    "        for (q,t) in train_qn_cls:\n",
    "            qn_epoch.append(q)\n",
    "            target_epoch.append(t)\n",
    "\n",
    "        \"\"\"\n",
    "        while done_chunks==False:\n",
    "            chunk_num+=1\n",
    "            \n",
    "            #we have covered ALL CLASS 0 items in this epoch, so break.\n",
    "            if start_chunk > lth_0:\n",
    "                done_chunks=True\n",
    "                break\n",
    "                \n",
    "            #this is going to be the last loop.\n",
    "            if end_chunk > lth_0:\n",
    "                end_chunk=lth_0\n",
    "                start_chunk=end_chunk-lth_1\n",
    "                done_chunks=True\n",
    "            print(\"start_chunk=\",start_chunk,\",end_chunk=\",end_chunk,\",done_chunks=\",done_chunks)\n",
    "            \n",
    "            arr0=class_0[start_chunk:end_chunk]\n",
    "            arr1=class_1\n",
    "            #print(\"arr=\",arr, \" arr1=\",arr1)\n",
    "            lst=np.concatenate((arr0,arr1))\n",
    "    \n",
    "            ### LET US SHUFFLE\n",
    "            for i in range (0,100):\n",
    "                np.random.shuffle(lst)\n",
    "            \n",
    "            lth=len(lst)\n",
    "            tokens_chunk=[qn for (qn,target) in lst]\n",
    "            targets_chunk=[target for (qn,target) in lst]\n",
    "            sequences_chunk=sequences_from_tokens(tokens_chunk)\n",
    "\n",
    "            data_len=len(sequences_chunk)\n",
    "            print(\"datalen=\",data_len)        \n",
    "            \n",
    "            batch_in_chunk=0\n",
    "            for i in range (0,data_len,BATCH_SIZE):                         \n",
    "                end_batch=i+BATCH_SIZE                \n",
    "                batch_input,batch_target=extract_batch_from_chunk(sequences_chunk,targets_chunk,i,end_batch)\n",
    "                ...\n",
    "            start_chunk=start_chunk+lth_1\n",
    "            end_chunk=start_chunk+lth_1\n",
    "\n",
    "        \"\"\"\n",
    "        data_len=len(qn_epoch)\n",
    "        for i in range (0,data_len,BATCH_SIZE):\n",
    "                end_batch=i+BATCH_SIZE\n",
    "                \n",
    "                batch_input,batch_target=extract_batch_from_chunk(qn_epoch,target_epoch,i,end_batch)\n",
    "                batch_input=sequences_from_tokens(batch_input)\n",
    "                #loss_num,num_correct,gru_h=train_loop(batch_input,batch_target,gru_h,cnt)\n",
    "                loss_num,num_correct,h,c=train_loop(batch_input,batch_target,h,c,i)\n",
    "                \n",
    "                \n",
    "                if i%2 == 0:\n",
    "                    myprint(\"epoch-\",epoch, # chunk=\",chunk_num,\" batch_in_chunk=\",batch_in_chunk,\n",
    "                        \" batch_in_epoch-\",cnt,                \n",
    "                      \" loss=\",loss_num,\n",
    "                      \", correctly predicted =\",num_correct)\n",
    "    \n",
    "                    \n",
    "                #epoch_accuracy += acc.item()\n",
    "                correct+=num_correct\n",
    "                epoch_loss+=loss_num\n",
    "        \n",
    "                cnt=cnt+1\n",
    "                #batch_in_chunk+=1\n",
    "                total+=len(batch_target)\n",
    "                \n",
    "                #if more than 5 hours, then break the training loop.\n",
    "                if ( (time.time() - start_time)/3600 >= time_limit ):\n",
    "                    myprint(\"Breaking the train loop as time exceeded limit.\")\n",
    "                    break\n",
    "\n",
    "        epoch_loss=epoch_loss / cnt\n",
    "        #epoch_accuracy= epoch_accuracy / cnt\n",
    "            \n",
    "        myprint( \"DONE. epoch-\",epoch,\" num_batches=\", cnt,\n",
    "          \" loss=\",epoch_loss, #\" accuracy=\", epoch_accuracy, \n",
    "          \" total=\",total,\n",
    "          \", correctly predicted=\",correct)\n",
    "    \n",
    "    #torch.save(model.state_dict(), \"model.pth\")\n",
    "    return h, c\n",
    "    #return model, optim, self.loss_fn,gru_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,c=train_fn(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Prediction will have these steps - \n",
    "1. normalize the test question texts.\n",
    "2. create sequences array\n",
    "3. predict using the trained model\n",
    "4. create output/result file submission.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(class_0)\n",
    "def test_fn(): \n",
    "    myprint(\"TEST STARTED\")\n",
    "    \n",
    "    ffile=open(\"submission.csv\",\"w\")\n",
    "    ffile.write(\"qid,prediction\\n\")\n",
    "    \n",
    "    #ffile_raw=open(\"test_raw_result.csv\",\"w\")\n",
    "    \n",
    "    #set the mode to eval.\n",
    "    model.eval()\n",
    "    \n",
    "    data_len=len(test_arr)\n",
    "\n",
    "    tokens=[qn for (qid,qn) in test_arr]\n",
    "    qids=[qid for (qid,qn) in test_arr]    \n",
    "    \n",
    "    for i in range (0,data_len,BATCH_SIZE):\n",
    "        end=i+BATCH_SIZE\n",
    "        \n",
    "        if end > data_len:\n",
    "            end=data_len\n",
    "        batch_input=sequences_from_tokens(tokens[i:end])\n",
    "                    \n",
    "        batch_qid=qids[i:end]\n",
    "                            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #pred,_=model(batch_input,None)\n",
    "            #we do not provide the h/c values that we obtained during train,\n",
    "            #as the last batch will not have the batch_size.\n",
    "            pred,_,_=model(batch_input,None,None)\n",
    "\n",
    "            #myprint(\"batch input=\",len(batch_input), \"preds=\",len(pred))\n",
    "            pred_sigmoid= torch.sigmoid(pred)\n",
    "            pred_rounded=model.round_prediction(pred_sigmoid)\n",
    "            \n",
    "            cnt=0\n",
    "            for qid in batch_qid:\n",
    "                lline=\"{},{}\\n\".format(qid,pred_rounded[cnt])            \n",
    "                ffile.write(lline)\n",
    "                \n",
    "                #lline=\"{},{}\\n\".format(qid,pred_sigmoid[cnt])\n",
    "                #ffile_raw.write(lline)\n",
    "                cnt+=1\n",
    "\n",
    "    ffile.close()\n",
    "    #ffile_raw.close()\n",
    "    print(\"DONE\")   \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
